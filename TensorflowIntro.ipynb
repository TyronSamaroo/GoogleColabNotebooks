{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorflowYoutube.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1jUfmOyhd0TzuGrGHjgPi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TyronSamaroo/GoogleColabNotebooks/blob/master/TensorflowIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V45_ntxaaubD",
        "colab_type": "text"
      },
      "source": [
        "# Author: Tyron Samaroo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPFYuSp6zGhz",
        "colab_type": "text"
      },
      "source": [
        "Referenced from\n",
        "\n",
        "https://www.youtube.com/watch?v=fNxaJsNG3-s&list=PLQY2H8rRoyvwLbzbnKJ59NkZvQAW9wLbx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-a63SV5zRbG",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing - Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgAVE_BXzWtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR6OmdUE0RFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9s6WSyZ0N9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizer object \n",
        "tokenizer = Tokenizer(num_words= 100)\n",
        "# Fix on given text\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ed3CU0O0GXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc2-SrkT0mts",
        "colab_type": "code",
        "outputId": "8a2f8a5b-2b7d-44d9-8565-e95a9628d3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Note that dog! is not outputed smart tokenizer stil doesnt make a new instance \n",
        "print(word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ2lcmJpzYB5",
        "colab_type": "text"
      },
      "source": [
        "# Sequencing - Turning sentences into data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWMxQjgj0rC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny_k8_no1YdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizer object \n",
        "tokenizer = Tokenizer(num_words= 100)\n",
        "# Fix on given text\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W76BU8MQ1lVx",
        "colab_type": "code",
        "outputId": "c9348e91-03ad-448f-9a20-f6f1c8d57410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuAX75yK1vJ6",
        "colab_type": "code",
        "outputId": "6d140786-453c-4f55-c7e6-37a86ef70b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TrM1hOo1oj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfzB6Tm42yVQ",
        "colab_type": "text"
      },
      "source": [
        "Be careful with data not in word index or present data. The corpus will not contain the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O6dnwyl1pE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = ['i really love my dog', 'my dog loves my ']\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNN-NKKA3isZ",
        "colab_type": "text"
      },
      "source": [
        "Notice that we have an issue. It doesnt map properly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GlKyJuR3Mdw",
        "colab_type": "code",
        "outputId": "6fcae934-774f-42d1-8a33-e2d4fc462c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(test_seq)\n",
        "print(word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1, 3], [1, 3, 1]]\n",
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfTrWmoS3gKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To fix \n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]\n",
        "tokenizer = Tokenizer(num_words= 100, oov_token= \"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm3A6NTp4A32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_seq = tokenizer.texts_to_sequences(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rub5l5eK4aoi",
        "colab_type": "code",
        "outputId": "8af25f20-0f36-42e4-fad0-c5cc257c5d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#Notice all new words that in test_data that was not in sentences is replaced with a token <OOV>. \n",
        "print(word_index)\n",
        "print(sequences)\n",
        "print(test_seq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrcCpdGsUYsj",
        "colab_type": "text"
      },
      "source": [
        "### Notice Change for Test Sequence when added new OOV token\n",
        "\n",
        "\n",
        "BEFORE \n",
        "\n",
        "> [[4, 2, 1, 3], [1, 3, 1]]\n",
        "\n",
        ">`{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}`\n",
        "\n",
        "\n",
        "### After \n",
        "> [[5, 1, 3, 2, 4], [2, 4, 1, 2]]\n",
        "\n",
        "> `{<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNDM2kubTASV",
        "colab_type": "text"
      },
      "source": [
        "Dealing with different size of text sequence. We use padding to help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCLd5jT64bmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDf8TrPl9l7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JBs-oXaTh9g",
        "colab_type": "code",
        "outputId": "bb21f6ad-f623-44f5-d5a1-e8083bde360d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unnXCBxeTvYy",
        "colab_type": "code",
        "outputId": "4b8ca483-8c2b-4cfa-d376-55b1b1e329f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xHorDBpVzKw",
        "colab_type": "text"
      },
      "source": [
        "Padding now makes all sentences the same length as inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRdvQ1rKTi73",
        "colab_type": "code",
        "outputId": "4eac314a-b082-490b-ce2a-c5331925f6bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(padded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  5  3  2  4]\n",
            " [ 0  0  0  5  3  2  7]\n",
            " [ 0  0  0  6  3  2  4]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlIwxBf6Wb-L",
        "colab_type": "text"
      },
      "source": [
        "### Other Parameters for Padding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY8mLt3HTkxr",
        "colab_type": "code",
        "outputId": "bab13c50-0752-4683-c76e-a1403d0c653d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "pad_sequences(sequences, maxlen=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  5,  3,  2,  4],\n",
              "       [ 0,  0,  0,  0,  0,  0,  5,  3,  2,  7],\n",
              "       [ 0,  0,  0,  0,  0,  0,  6,  3,  2,  4],\n",
              "       [ 0,  0,  0,  8,  6,  9,  2,  4, 10, 11]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlEb4MQ9WrJY",
        "colab_type": "code",
        "outputId": "93915f18-9ec0-4ad5-ae4e-ef57258b57b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "pad_sequences(sequences, maxlen=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  2,  4],\n",
              "       [ 3,  2,  7],\n",
              "       [ 3,  2,  4],\n",
              "       [ 4, 10, 11]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y9yjcEAXY1p",
        "colab_type": "code",
        "outputId": "b1788316-1ff8-4fb3-d07f-c8c25fa9cf11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "pad_sequences(sequences, padding='post')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5,  3,  2,  4,  0,  0,  0],\n",
              "       [ 5,  3,  2,  7,  0,  0,  0],\n",
              "       [ 6,  3,  2,  4,  0,  0,  0],\n",
              "       [ 8,  6,  9,  2,  4, 10, 11]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCG1POevXiCK",
        "colab_type": "code",
        "outputId": "6fc28379-34a4-4557-9677-00a20547abe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "pad_sequences(sequences, truncating='pre', maxlen=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  5,  3,  2,  4],\n",
              "       [ 0,  5,  3,  2,  7],\n",
              "       [ 0,  6,  3,  2,  4],\n",
              "       [ 9,  2,  4, 10, 11]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx-Nc8oIY4V8",
        "colab_type": "text"
      },
      "source": [
        "# Training a model to reconize sentiment in text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwmieJSGY8pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iHvVBHcbqS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}